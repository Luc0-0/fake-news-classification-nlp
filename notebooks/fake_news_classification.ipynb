{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classification using Classical NLP\n",
    "\n",
    "Binary classification of news articles as either fake or factual using classical natural language processing techniques and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The task is to classify news articles into two categories:\n",
    "- **Fake News**: Intentionally misleading or false information\n",
    "- **Factual News**: Verified, factual reporting\n",
    "\n",
    "This problem is important for information verification, media literacy, and combating misinformation online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Approach\n",
    "1. **Data Loading**: Load news articles with labels\n",
    "2. **Text Processing**: Tokenization, stemming, stopword removal\n",
    "3. **Feature Extraction**: Bag-of-Words (TF-IDF) representation\n",
    "4. **Model Training**: Train two classical classifiers\n",
    "5. **Evaluation**: Compare performance using standard metrics\n",
    "\n",
    "### Models Used\n",
    "- **Logistic Regression**: Linear classification with probabilistic output\n",
    "- **Linear SVM (SGDClassifier)**: Support Vector Machine with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Configure visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "default_color = '#00bfbf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_excel(\"../data/fake_news_data.xlsx\", engine=\"openpyxl\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {data.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(data['fake_or_factual'].value_counts())\n",
    "\n",
    "# Visualize distribution\n",
    "data['fake_or_factual'].value_counts().plot(kind='bar', color=default_color, title='News Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and stem\n",
    "    tokens = [stemmer.stem(token) for token in tokens \n",
    "              if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nOriginal: {data['text'].iloc[0][:200]}...\")\n",
    "print(f\"\\nProcessed: {data['processed_text'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.8)\n",
    "X = vectorizer.fit_transform(data['processed_text'])\n",
    "\n",
    "# Prepare labels\n",
    "y = (data['fake_or_factual'] == 'Fake News').astype(int)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Class distribution - Fake: {(y == 1).sum()}, Factual: {(y == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Training set - Fake: {(y_train == 1).sum()}, Factual: {(y_train == 0).sum()}\")\n",
    "print(f\"Test set - Fake: {(y_test == 1).sum()}, Factual: {(y_test == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Train Linear SVM\n",
    "print(\"Training Linear SVM (SGDClassifier)...\")\n",
    "svm_model = SGDClassifier(loss='hinge', random_state=42, n_jobs=-1, max_iter=1000)\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and print metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} Performance\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Factual', 'Fake']))\n",
    "    \n",
    "    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
    "\n",
    "# Evaluate both models\n",
    "lr_results = evaluate_model(lr_model, X_test, y_test, \"Logistic Regression\")\n",
    "svm_results = evaluate_model(svm_model, X_test, y_test, \"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Logistic Regression': lr_results,\n",
    "    'Linear SVM': svm_results\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "comparison_df.T.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim([0.6, 1.0])\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_model.predict(X_test))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Factual', 'Fake'], yticklabels=['Factual', 'Fake'])\n",
    "axes[0].set_title('Logistic Regression')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Linear SVM confusion matrix\n",
    "svm_cm = confusion_matrix(y_test, svm_model.predict(X_test))\n",
    "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Factual', 'Fake'], yticklabels=['Factual', 'Fake'])\n",
    "axes[1].set_title('Linear SVM')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Best Performing Model\n",
    "\n",
    "Both Logistic Regression and Linear SVM performed well on this task. The models successfully classify fake vs. factual news with high accuracy.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Feature Importance**: Words related to politics, government, and elections are more common in fake news\n",
    "2. **Class Balance**: The dataset is reasonably balanced between fake and factual news\n",
    "3. **TF-IDF Features**: Using top 5000 features provided good discrimination power\n",
    "4. **Simple Models Effective**: Classical ML models outperform complex approaches for this task with proper feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "1. **Ensemble Methods**: Combine multiple models using voting or stacking\n",
    "2. **Hyperparameter Tuning**: Use GridSearchCV or RandomizedSearchCV for optimization\n",
    "3. **Feature Engineering**: Include additional NLP features like sentiment, readability metrics\n",
    "4. **Domain-Specific Models**: Fine-tune on news-specific datasets\n",
    "5. **Explainability**: Analyze which features most influence predictions\n",
    "6. **Cross-validation**: Use k-fold cross-validation for more robust evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates that classical NLP techniques combined with simple machine learning classifiers can effectively distinguish between fake and factual news. While modern deep learning approaches exist, this classical approach provides interpretability, computational efficiency, and competitive performance on this binary classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
